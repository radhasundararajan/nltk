{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74646376-6e27-4506-847b-9649647f627f",
   "metadata": {},
   "source": [
    "# Import pandas and http requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1b600412-9e97-4770-aca2-157fc9741199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "25a5a643-1656-4a82-8723-ac83d2af1ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contains files Youtube01-Psy.csv, Youtube02-KatyPerry.csv, YouTube03-LMFAO.csv, Youtube04-Eminem.csv, Youtube05-Shakira.csv\n",
    "import zipfile\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# URL of the dataset\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00380/YouTube-Spam-Collection-v1.zip'\n",
    "zip_file = 'YouTube-Spam-Collection-v1.zip'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d21cb6-abab-43c6-8e8f-70f3281174ea",
   "metadata": {},
   "source": [
    "# Download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8b711247-18a5-44fe-9ab2-5bd42d95f1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = requests.get(url)\n",
    "with open(zip_file, 'wb') as file:\n",
    "    file.write(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35434e44-7557-4c90-871e-4cca716cfdd4",
   "metadata": {},
   "source": [
    "# Extract the ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "c546a4d2-005a-41e0-b05f-5477c357ca2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall('youtube_spam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bac09285-0dab-4980-bc96-2c484c7294d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files: ['Youtube03-LMFAO.csv', 'Youtube04-Eminem.csv', 'Dataset', 'Youtube05-Shakira.csv', 'Youtube02-KatyPerry.csv', '__MACOSX', 'Youtube01-Psy.csv']\n"
     ]
    }
   ],
   "source": [
    "# List the extracted files\n",
    "extracted_files = os.listdir('youtube_spam')\n",
    "print('Extracted files:', extracted_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed88454-b0fb-487f-94be-d3354b9d1c50",
   "metadata": {},
   "source": [
    "# Read each CSV file into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "4e9905c0-4352-4d4e-a7b2-0af6dc924ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE is the individual csv file -->  Youtube03-LMFAO.csv\n",
      "['COMMENT_ID', 'AUTHOR', 'DATE', 'CONTENT', 'CLASS']\n",
      "HERE is the individual csv file -->  Youtube04-Eminem.csv\n",
      "['COMMENT_ID', 'AUTHOR', 'DATE', 'CONTENT', 'CLASS']\n",
      "HERE is the individual csv file -->  Youtube05-Shakira.csv\n",
      "['COMMENT_ID', 'AUTHOR', 'DATE', 'CONTENT', 'CLASS']\n",
      "HERE is the individual csv file -->  Youtube02-KatyPerry.csv\n",
      "['COMMENT_ID', 'AUTHOR', 'DATE', 'CONTENT', 'CLASS']\n",
      "HERE is the individual csv file -->  Youtube01-Psy.csv\n",
      "['COMMENT_ID', 'AUTHOR', 'DATE', 'CONTENT', 'CLASS']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfs = []\n",
    "fbContent = []\n",
    "\n",
    "for file in extracted_files:\n",
    "    if file.endswith('.csv'):\n",
    "        print(\"HERE is the individual csv file --> \", file)\n",
    "    ## Open the CSV file in read mode\n",
    "        try:\n",
    "            with open(os.path.join('youtube_spam', file), 'r') as csvfile:\n",
    "          # Create a CSV reader object\n",
    "                csv_reader = csv.reader(csvfile)\n",
    "                df = pd.read_csv(os.path.join('youtube_spam', file))\n",
    "                headings=df.columns.tolist()\n",
    "                print(headings)\n",
    "                dfs.append(df)\n",
    "        except:\n",
    "            print(\"unable to read the file\", file)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0770ef-2aec-4052-8dff-199a5fc2f715",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e7742002-afa0-4525-ae3f-faf772769c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              COMMENT_ID        AUTHOR  \\\n",
      "0  z13uwn2heqndtr5g304ccv5j5kqqzxjadmc0k  Corey Wilson   \n",
      "1  z124jvczaz3dxhnbc04cffk43oiugj25yzo0k   Epic Gaming   \n",
      "2      z13tczjy5xj0vjmu5231unho1ofey5zdk     LaS Music   \n",
      "3  z13tzr0hdpnayhqqc04cd3zqqqjkf3ngckk0k    Cheryl Fox   \n",
      "4  z12pcvix4zedcjvyb04ccr1r0mr2g5xwyng0k    PATRICK_TW   \n",
      "\n",
      "                         DATE  \\\n",
      "0  2015-05-28T21:39:52.376000   \n",
      "1  2015-05-28T20:07:20.610000   \n",
      "2  2015-05-28T19:23:35.355000   \n",
      "3  2015-05-28T17:49:35.294000   \n",
      "4  2015-05-28T16:28:26.818000   \n",
      "\n",
      "                                             CONTENT  CLASS  \n",
      "0  <a href=\"http://www.youtube.com/watch?v=KQ6zr6...      0  \n",
      "1                                   wierd but funny﻿      0  \n",
      "2  Hey guys, I&#39;m a human.<br /><br /><br />Bu...      1  \n",
      "3       Party Rock....lol...who wants to shuffle!!!﻿      0  \n",
      "4                                        Party rock﻿      0  \n"
     ]
    }
   ],
   "source": [
    "combinedDataFrame = pd.concat(dfs, ignore_index=True)\n",
    "print(combinedDataFrame.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fda3415-f0d6-4f25-bf0f-e120d3348d5b",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077176e2-f412-408e-9ec0-9918d1039a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ed1fe179-d5c4-4f56-9459-ed15e6075f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['CONTENT'].str.split().tolist() converts the column into a list of lists.\n",
    "#sum(..., []) flattens the list of lists into a single list.\n",
    "# select only those data which are not spam by shortlisting on CLASS=0\n",
    "nonSpamdf = combinedDataFrame[combinedDataFrame['CLASS'] == 0 ]\n",
    "all_words = sum(nonSpamdf['CONTENT'].str.split().tolist(), [])\n",
    "#all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "cdd3f11d-1d37-4414-a56d-48db9004bc13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/radha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "150d79d8-9ef0-414d-b9a6-b31d9247a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove stopwords\n",
    "filtered_list=[]\n",
    "for word in all_words:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b08f24-b36a-4676-84b6-ec8fe2e84cad",
   "metadata": {},
   "source": [
    "#filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "622314dc-88d9-4889-941e-854c8768f505",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "44d1b6e9-ce64-4d04-991e-670105e2e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatize the words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_list]\n",
    "#lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "1ca9419b-d565-4fab-a723-4d11c339b0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 2587 samples and 5294 outcomes>\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "frequency_distribution = FreqDist(lemmatized_words)\n",
    "print(frequency_distribution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "219d5739-ce07-4cd0-a7ed-215e628b1186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('song', 149),\n",
       " ('love', 100),\n",
       " ('\\ufeff', 80),\n",
       " ('like', 79),\n",
       " ('video', 61),\n",
       " ('view', 41),\n",
       " ('2', 41),\n",
       " ('Katy', 40),\n",
       " ('billion', 35),\n",
       " ('best', 33),\n",
       " ('get', 30),\n",
       " ('people', 29),\n",
       " ('song\\ufeff', 28),\n",
       " ('Love', 27),\n",
       " ('year', 26),\n",
       " ('know', 24),\n",
       " ('good', 24),\n",
       " ('-', 23),\n",
       " ('/><br', 23),\n",
       " ('still', 21)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_distribution.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a516ab1-0e7e-4ca1-80df-246135a3692f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
